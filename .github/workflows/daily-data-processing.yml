name: Daily Data Processing Pipeline

# Description: Automated daily data processing pipeline that processes input CSV files
# from /data/input/ and outputs processed results to /data/output/.
# Scheduled to run daily at 10:36 AM for continuous data transformation and analysis.

on:
  schedule:
    # Run daily at 10:36 AM EST (15:36 UTC in winter, 14:36 UTC in summer - using 15:36 UTC)
    - cron: '36 15 * * *'
  workflow_dispatch:  # Allow manual trigger

env:
  WORKSPACE_URL: 'https://your-databricks-workspace.cloud.databricks.com'
  NOTEBOOK_PATH: '/Workspace/Users/scarmonit@gmail.com/Sample_Data_Processing_Notebook'
  INPUT_PATH: '/data/input/sample_data.csv'
  OUTPUT_PATH: '/data/output/processed_data.csv'
  NOTIFICATION_EMAIL: 'scarmonit@gmail.com'

jobs:
  data-processing:
    name: Data Processing Task
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max timeout for safety (changed from 0 which is invalid)
    
    # Tags for the workflow
    env:
      ENVIRONMENT: Production
      TYPE: Automated
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1 - pinned to SHA
      
      # ENHANCEMENT 1: Add health check and input validation
      - name: Validate environment configuration
        run: |
          echo "=== Configuration Validation ==="
          echo "Workspace URL: ${{ env.WORKSPACE_URL }}"
          echo "Notebook Path: ${{ env.NOTEBOOK_PATH }}"
          echo "Input Path: ${{ env.INPUT_PATH }}"
          echo "Output Path: ${{ env.OUTPUT_PATH }}"
          echo "Notification Email: ${{ env.NOTIFICATION_EMAIL }}"
          
          # Validate required secrets are set
          if [ -z "${{ secrets.DATABRICKS_TOKEN }}" ]; then
            echo "ERROR: DATABRICKS_TOKEN secret is not set"
            exit 1
          fi
          
          if [ -z "${{ secrets.EMAIL_USERNAME }}" ]; then
            echo "ERROR: EMAIL_USERNAME secret is not set"
            exit 1
          fi
          
          if [ -z "${{ secrets.EMAIL_PASSWORD }}" ]; then
            echo "ERROR: EMAIL_PASSWORD secret is not set"
            exit 1
          fi
          
          echo "‚úì All secrets validated successfully"
      
      - name: Send start notification email
        if: always()
        uses: dawidd6/action-send-mail@2cea9617b09d79a095af21254fbcb7ae95903dde  # v3.12.0 - pinned to SHA
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: 'üöÄ Pipeline Started: Daily Data Processing'
          to: ${{ env.NOTIFICATION_EMAIL }}
          from: 'GitHub Actions <noreply@github.com>'
          body: |
            Daily Data Processing Pipeline has started.
            
            Job: ${{ github.job }}
            Workflow: ${{ github.workflow }}
            Run ID: ${{ github.run_id }}
            Run URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            Triggered by: ${{ github.event_name }}
            Repository: ${{ github.repository }}
            Branch: ${{ github.ref_name }}
            
            Configuration:
            - Workspace: ${{ env.WORKSPACE_URL }}
            - Notebook: ${{ env.NOTEBOOK_PATH }}
            - Input: ${{ env.INPUT_PATH }}
            - Output: ${{ env.OUTPUT_PATH }}
            
            Timestamp: ${{ github.event.head_commit.timestamp }}
      
      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d  # v5.1.0 - pinned to SHA
        with:
          python-version: '3.11'  # Specify exact version instead of '3.x'
      
      - name: Install Databricks CLI
        run: |
          echo "=== Installing Databricks CLI ==="
          pip install --upgrade pip
          pip install databricks-cli
          databricks --version
          echo "‚úì Databricks CLI installed successfully"
      
      - name: Configure Databricks CLI
        run: |
          echo "=== Configuring Databricks CLI ==="
          echo "[DEFAULT]" > ~/.databrickscfg
          echo "host = ${{ env.WORKSPACE_URL }}" >> ~/.databrickscfg
          echo "token = ${{ secrets.DATABRICKS_TOKEN }}" >> ~/.databrickscfg
          chmod 600 ~/.databrickscfg
          echo "‚úì Databricks CLI configured successfully"
      
      # ENHANCEMENT 2: Add connection health check before execution
      - name: Test Databricks connectivity
        run: |
          echo "=== Testing Databricks Connection ==="
          if databricks workspace list / --profile DEFAULT > /dev/null 2>&1; then
            echo "‚úì Successfully connected to Databricks workspace"
          else
            echo "ERROR: Failed to connect to Databricks workspace"
            echo "Please verify DATABRICKS_TOKEN and WORKSPACE_URL"
            exit 1
          fi
      
      # ENHANCEMENT 3: Enhanced logging and error handling
      - name: Execute Data Processing Notebook (with retries)
        id: notebook_execution
        uses: nick-fields/retry@7152eba30c6575329ac0576536151eff5711479f  # v3.0.0 - pinned to SHA
        with:
          timeout_minutes: 300  # 5 hours max per attempt
          max_attempts: 3
          retry_wait_seconds: 300  # 5 minutes between retries (300 seconds)
          retry_on: error
          command: |
            echo "=== Starting Data Processing Execution ==="
            echo "Attempt: ${{ github.run_attempt }}"
            echo "Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)"
            
            # Execute the notebook using Databricks CLI
            echo "Exporting notebook from workspace..."
            databricks workspace export \
              "${{ env.NOTEBOOK_PATH }}" \
              /tmp/notebook.py
            
            if [ ! -f /tmp/notebook.py ]; then
              echo "ERROR: Failed to export notebook"
              exit 1
            fi
            
            echo "‚úì Notebook exported successfully"
            
            # Run the notebook with parameters
            echo "=== Processing data with parameters ==="
            echo "Input Path: ${{ env.INPUT_PATH }}"
            echo "Output Path: ${{ env.OUTPUT_PATH }}"
            
            RUN_NAME="Daily_Data_Processing_$(date +%Y%m%d_%H%M%S)"
            echo "Run Name: $RUN_NAME"
            
            # Submit the job and capture output
            echo "Submitting Databricks job..."
            SUBMIT_OUTPUT=$(databricks runs submit \
              --run-name "$RUN_NAME" \
              --notebook-task '{
                "notebook_path": "'"${{ env.NOTEBOOK_PATH }}"'",
                "base_parameters": {
                  "input_path": "'"${{ env.INPUT_PATH }}"'",
                  "output_path": "'"${{ env.OUTPUT_PATH }}"'"
                },
                "source": "WORKSPACE"
              }' \
              --new-cluster '{
                "spark_version": "13.3.x-scala2.12",
                "node_type_id": "i3.xlarge",
                "num_workers": 2,
                "spark_conf": {
                  "spark.databricks.delta.preview.enabled": "true"
                }
              }' 2>&1)
            
            echo "$SUBMIT_OUTPUT"
            
            # Extract run ID from output
            RUN_ID=$(echo "$SUBMIT_OUTPUT" | grep -oP '"run_id":\s*\K[0-9]+')
            
            if [ -z "$RUN_ID" ]; then
              echo "ERROR: Failed to submit job or extract run ID"
              exit 1
            fi
            
            echo "‚úì Job submitted successfully with Run ID: $RUN_ID"
            echo "Monitoring job execution..."
            
            # Monitor job status
            MAX_WAIT=14400  # 4 hours in seconds
            ELAPSED=0
            POLL_INTERVAL=30
            
            while [ $ELAPSED -lt $MAX_WAIT ]; do
              STATUS_OUTPUT=$(databricks runs get --run-id $RUN_ID 2>&1)
              LIFE_CYCLE_STATE=$(echo "$STATUS_OUTPUT" | grep -oP '"life_cycle_state":\s*"\K[^"]+' | head -1)
              RESULT_STATE=$(echo "$STATUS_OUTPUT" | grep -oP '"result_state":\s*"\K[^"]+' | head -1)
              
              echo "[$(date -u +%H:%M:%S)] Status: $LIFE_CYCLE_STATE | Result: $RESULT_STATE"
              
              if [ "$LIFE_CYCLE_STATE" = "TERMINATED" ]; then
                if [ "$RESULT_STATE" = "SUCCESS" ]; then
                  echo "‚úì Job completed successfully!"
                  exit 0
                else
                  echo "ERROR: Job failed with result state: $RESULT_STATE"
                  exit 1
                fi
              fi
              
              sleep $POLL_INTERVAL
              ELAPSED=$((ELAPSED + POLL_INTERVAL))
            done
            
            echo "ERROR: Job execution timeout after $MAX_WAIT seconds"
            exit 1
      
      # ENHANCEMENT 4: Success notification with details
      - name: Send success notification email
        if: success()
        uses: dawidd6/action-send-mail@2cea9617b09d79a095af21254fbcb7ae95903dde  # v3.12.0 - pinned to SHA
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: '‚úÖ Pipeline Succeeded: Daily Data Processing'
          to: ${{ env.NOTIFICATION_EMAIL }}
          from: 'GitHub Actions <noreply@github.com>'
          body: |
            Daily Data Processing Pipeline completed successfully!
            
            Job: ${{ github.job }}
            Workflow: ${{ github.workflow }}
            Run ID: ${{ github.run_id }}
            Run URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            Configuration:
            - Input: ${{ env.INPUT_PATH }}
            - Output: ${{ env.OUTPUT_PATH }}
            
            Timestamp: ${{ github.event.head_commit.timestamp }}
      
      - name: Send failure notification email
        if: failure()
        uses: dawidd6/action-send-mail@2cea9617b09d79a095af21254fbcb7ae95903dde  # v3.12.0 - pinned to SHA
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: '‚ùå Pipeline Failed: Daily Data Processing'
          to: ${{ env.NOTIFICATION_EMAIL }}
          from: 'GitHub Actions <noreply@github.com>'
          body: |
            Daily Data Processing Pipeline has FAILED.
            
            Job: ${{ github.job }}
            Workflow: ${{ github.workflow }}
            Run ID: ${{ github.run_id }}
            Run URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            Attempt: ${{ github.run_attempt }}
            
            Please check the logs for details.
            
            Configuration:
            - Workspace: ${{ env.WORKSPACE_URL }}
            - Notebook: ${{ env.NOTEBOOK_PATH }}
            - Input: ${{ env.INPUT_PATH }}
            - Output: ${{ env.OUTPUT_PATH }}
            
            Timestamp: ${{ github.event.head_commit.timestamp }}
      
      - name: Send skipped/cancelled notification email
        if: cancelled()
        uses: dawidd6/action-send-mail@2cea9617b09d79a095af21254fbcb7ae95903dde  # v3.12.0 - pinned to SHA
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.EMAIL_USERNAME }}
          password: ${{ secrets.EMAIL_PASSWORD }}
          subject: '‚ö†Ô∏è Pipeline Cancelled: Daily Data Processing'
          to: ${{ env.NOTIFICATION_EMAIL }}
          from: 'GitHub Actions <noreply@github.com>'
          body: |
            Daily Data Processing Pipeline was CANCELLED.
            
            Job: ${{ github.job }}
            Workflow: ${{ github.workflow }}
            Run ID: ${{ github.run_id }}
            Run URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            Timestamp: ${{ github.event.head_commit.timestamp }}

concurrency:
  group: data-processing-pipeline
  cancel-in-progress: false  # Ensures max 1 concurrent run
